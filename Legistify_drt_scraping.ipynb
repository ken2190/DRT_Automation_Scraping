{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ace785f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing Necessary Modules\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import sys\n",
    "import cv2\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By \n",
    "import pytesseract\n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "\n",
    "driver = None\n",
    "\n",
    "#Opening the Website\n",
    "def drt_login():\n",
    "    global driver\n",
    "    Link = \"https://drt.gov.in/front/page1_advocate.php\"\n",
    "    driver = webdriver.Chrome()\n",
    "    #wait = WebDriverWait(driver, 600)\n",
    "    driver.get(Link)\n",
    "    driver.maximize_window()\n",
    "    \n",
    "    \n",
    "#Selecting values of DRT/ DRAT name and Party name to get data    \n",
    "def drt_select():\n",
    "    partyname = '/html/body/div[1]/div/form/div[1]/div[4]/a'\n",
    "    drt_xpath = '//select[@id=\"schemaname\"]'\n",
    "    drt_chennai_xpath = '//*[@id=\"schemaname\"]/option[3]'\n",
    "    driver.find_element(By.XPATH,partyname).click()\n",
    "    driver.find_element(By.XPATH,drt_xpath).click()\n",
    "    driver.find_element(By.XPATH,drt_chennai_xpath).click()\n",
    "    \n",
    "    drt_party = 'sha'\n",
    "    driver.find_element(By.ID,'name').send_keys(drt_party)\n",
    "    \n",
    "    \n",
    "#Entering captcha value received by pytesseract    \n",
    "def bypass_captcha():\n",
    "    #Downloading Captcha image\n",
    "    with open('.//captcha.png', 'wb') as file:\n",
    "        file.write(driver.find_element_by_xpath('//*[@id=\"captchatext\"]/img').screenshot_as_png) \n",
    "        \n",
    "    #Image noise cleaning for obtaining better results\n",
    "    #This is not very accurate in obtaining captchas yet\n",
    "    img = cv2.imread(\"captcha.png\")\n",
    "    gry = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    (h, w) = gry.shape[:2]\n",
    "    gry = cv2.resize(gry, (w*2, h*2))\n",
    "    cls = cv2.morphologyEx(gry, cv2.MORPH_CLOSE, None)\n",
    "    thr = cv2.threshold(cls, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]\n",
    "    txt = pytesseract.image_to_string(thr).strip()\n",
    "    driver.find_element_by_xpath('//*[@id=\"captchatext\"]/input').send_keys(txt)\n",
    "    driver.find_element_by_id('submit1').click()\n",
    "    \n",
    "    \n",
    "#Scraping the Main report page   \n",
    "def scrape_main():\n",
    "   # with open(\"source.text\",\"w\") as file:\n",
    "    #    file.write(driver.page_source)\n",
    "    th_list = driver.find_elements_by_tag_name('th')\n",
    "    headers = [x.text for x in th_list[1:]]\n",
    "    td_list = driver.find_elements_by_tag_name('td')\n",
    "    data = [x.text for x in td_list]\n",
    "    table_rows = []\n",
    "    r = []\n",
    "    count = 0\n",
    "    for i in range(len(data)):\n",
    "        count += 1\n",
    "        r.append(data[i])\n",
    "        if count == len(headers):\n",
    "            table_rows.append(r)\n",
    "            r = []\n",
    "            count = 0\n",
    "    return headers,table_rows\n",
    "\n",
    "#Converting first page to Dataframe and storing in \"df\" pandas dataframe object\n",
    "def convert_to_df(columns,rows):\n",
    "    df = pd.DataFrame(rows, columns=columns)\n",
    "    return df\n",
    "\n",
    "#Scraping URLs from the \"More Details\" Hyperlink\n",
    "#Updating the original table with links for the Case details to be accessed anytime\n",
    "def scrape_detail_url(df, detail_url):\n",
    "    urls = []\n",
    "    for i in range(1,len(df.index)+1):\n",
    "        detail_xpath = \"/html/body/div[1]/div/form/div[5]/div/div[2]/table/tbody/tr[{}]/td[9]/a\".format(i)\n",
    "        attr = driver.find_element_by_xpath(detail_xpath).get_attribute('href')\n",
    "        url = detail_url + attr.split(\"'\",2)[1]\n",
    "        urls.append(url)\n",
    "    df = df.drop('View More', 1)\n",
    "    df[\"Link for More Details\"] = urls\n",
    "    driver.close()\n",
    "        \n",
    "        \n",
    "    return df\n",
    "\n",
    "#Scraping first table in all links for Case details\n",
    "#cleaning the list to look more presentable\n",
    "def scrape_table1_details(df):\n",
    "    datalist = []\n",
    "\n",
    "    browser = webdriver.Chrome()\n",
    "    #wait = WebDriverWait(driver, 600)\n",
    "    for url in df[\"Link for More Details\"].tolist():\n",
    "        browser.get(url)\n",
    "        #browser.maximize_window()\n",
    "        \n",
    "        tables = browser.find_elements_by_tag_name('table')\n",
    "        #table1\n",
    "        tablerows = tables[0].find_elements_by_tag_name('tr')\n",
    "        t = []\n",
    "        for tr in tablerows:\n",
    "            if tr.find_elements_by_tag_name('th') != []:\n",
    "                continue\n",
    "                #t.append(tr.find_element_by_tag_name('th').text)\n",
    "            if tr.find_elements_by_tag_name('td') != []:\n",
    "                td_data = []\n",
    "                if len(tr.find_elements_by_tag_name('td')) == 1:\n",
    "                    continue\n",
    "                for td in tr.find_elements_by_tag_name('td'):\n",
    "                    td_data.append(td.text)\n",
    "                t.append(td_data)\n",
    "        \n",
    "        datalist.extend(t)\n",
    "    browser.close()\n",
    "    table1_data = []\n",
    "    columns = ['Diary no/Year','Case Type/Case No/Year','DRT Detail','Date of Filing.','Case Status.','In the Court of','Court No.',\n",
    "               'Next Listing Date','Next Listing Purpose']\n",
    "    for i in range(len(columns)):\n",
    "        temp = [columns[i]]\n",
    "        for j in range(len(datalist)):\n",
    "            if datalist[j][0] == columns[i]:\n",
    "                temp.append(datalist[j][1])\n",
    "        table1_data.append(temp)\n",
    "        \n",
    "    \n",
    "    table1_df = pd.DataFrame(table1_data).T\n",
    "    table1_df = table1_df.rename(columns=table1_df.iloc[0]).drop(table1_df.index[0])       \n",
    "    return table1_df\n",
    "\n",
    "#Scraping second table in all links for Petitioner details\n",
    "#cleaning the list to look more presentable\n",
    "def scrape_table2_details(df):\n",
    "    datalist = []\n",
    "    diary = []\n",
    "    browser = webdriver.Chrome()\n",
    "    #wait = WebDriverWait(driver, 600)\n",
    "    for url in df[\"Link for More Details\"].tolist():\n",
    "        browser.get(url)\n",
    "        #browser.maximize_window()\n",
    "        diary.append(browser.find_element_by_xpath('/html/body/div/form/font/table[1]/tbody/tr[2]/td[2]').text)\n",
    "        tables = browser.find_elements_by_tag_name('table')\n",
    "        #table2\n",
    "        tablerows = tables[1].find_elements_by_tag_name('tr')\n",
    "        data = [x.text for x in tablerows]\n",
    "        datalist.append(data)\n",
    "    browser.close()\n",
    "    table2_data = []\n",
    "    columns = ['PETITIONER/APPLICANT DETAIL','RESPONDENTS/DEFENDENT DETAILS']\n",
    "    temp = []\n",
    "    for data in datalist:\n",
    "        temp = [data[1],data[3]]      \n",
    "        table2_data.append(temp)\n",
    "    \n",
    "    table2_df = pd.DataFrame(table2_data)\n",
    "    table2_df.columns = columns\n",
    "    table2_df.insert(loc = 0, column = 'Diary no/Year', value = diary)\n",
    "    return table2_df\n",
    "\n",
    "#Scraping third table in all links for Case Proceedings details\n",
    "#cleaning the list to look more presentable\n",
    "def scrape_table3_details(df):\n",
    "    datalist = []\n",
    "    diary = []\n",
    "    browser = webdriver.Chrome()\n",
    "    #wait = WebDriverWait(driver, 600)\n",
    "    for url in df[\"Link for More Details\"].tolist():\n",
    "        browser.get(url)\n",
    "        #browser.maximize_window()\n",
    "        diary.append(browser.find_element_by_xpath('/html/body/div/form/font/table[1]/tbody/tr[2]/td[2]').text)\n",
    "        tables = browser.find_elements_by_tag_name('table')\n",
    "        #table3\n",
    "        tablerows = tables[2].find_elements_by_tag_name('tr')\n",
    "        data = [x.text for x in tablerows]\n",
    "        if len(data) > 3:\n",
    "            datalist.append(data[2:-1])\n",
    "        else:\n",
    "            datalist.append([\"None None None\"])        \n",
    "    browser.close()\n",
    "    table3_data = []\n",
    "    for i in range(len(diary)):\n",
    "        for data in datalist[i]:\n",
    "            table3_data.append(str(diary[i] + \" \" + data).split(\" \",3))\n",
    "    columns = ['Diary no/Year','Court Name','Causelist Date','Purpose']\n",
    "    \n",
    "    \n",
    "    table3_df = pd.DataFrame(table3_data)\n",
    "    table3_df.columns = columns\n",
    "    \n",
    "    return table3_df\n",
    "\n",
    "\n",
    "#Pushing the dataframes to database.\n",
    "#If the database already exists, updating the database with the newer values\n",
    "\n",
    "\n",
    "def postgres_storing(df,table1_df, table2_df, table3_df):\n",
    "    db = create_engine('postgresql://postgres:db@localhost:5432/Legistify')\n",
    "    con = db.connect()\n",
    "    df.to_sql(\"Report\",con,if_exists='replace')\n",
    "    table1_df.to_sql(\"Case Status\",con,if_exists='replace')\n",
    "    table2_df.to_sql(\"Petitioner Details\",con,if_exists='replace')\n",
    "    table3_df.to_sql(\"Case Proceedings Details\",con,if_exists='replace')\n",
    "        \n",
    "           \n",
    "if __name__==\"__main__\":\n",
    "    drt_login()     \n",
    "    \n",
    "    drt_select()\n",
    "    \n",
    "    try:\n",
    "        bypass_captcha()\n",
    "    except:\n",
    "        print(\"Invalid Captcha. Please try again\")\n",
    "        sys.exit()\n",
    "    \n",
    "    columns, rows = scrape_main()\n",
    "    \n",
    "    \n",
    "    df = convert_to_df(columns, rows)\n",
    "    \n",
    "    detail_url = \"https://drt.gov.in/drtlive/Misdetailreport.php?no=\"\n",
    "    \n",
    "    df = scrape_detail_url(df,detail_url)\n",
    "    \n",
    "    table1_df = scrape_table1_details(df)\n",
    "    \n",
    "    table2_df = scrape_table2_details(df)\n",
    "    \n",
    "    table3_df = scrape_table3_details(df)\n",
    "    \n",
    "    postgres_storing(df,table1_df, table2_df, table3_df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e745ac6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2177fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665db7b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73ce03c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc72827b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef33d17c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef91c6ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ee23cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981ee402",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93082b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
